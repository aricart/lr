# LocalRag (lr)

A local-first retrieval-augmented generation (rag) system for indexing and
querying code repositories and documentation.

All the code in this project is AI generated by
[claude code](https://claude.ai). Of course all the ideas on how to make it work
are mine. The idea is that this can enhance development of code written by
humans or tools with factual and almost hallucination-free constructs.

## why localrag?

- **privacy-first**: all data stays on your machine, no code sent to third
  parties
- **flexible**: index any repository - your code, open source projects,
  documentation
- **simple**: single binary, no databases or complex dependencies
- **code-aware**: intelligent chunking that understands functions and methods
- **multi-source**: query across multiple indexed repositories at once
- **customizable**: supports multiple embedding and llm providers

## features

- index local directories or git repositories
- intelligent code-aware chunking for go, javascript, typescript, templ, python, java, and c
- markdown documentation support
- multiple embedding providers:
  - openai (text-embedding-3-small)
  - voyage ai (code-optimized embeddings)
- multiple llm providers:
  - openai (gpt-4o-mini)
  - anthropic claude (sonnet 4)
- compressed index storage (.lrindex format with gzip)
- xdg base directory compliance for data and config
- multi-source search (query across all indexed repos)
- interactive cli mode
- model context protocol (mcp) server for ai agent integration
- checkpoint/resume support for long indexing jobs

## architecture

localrag is built around a simple but powerful pipeline:

```
┌─────────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐
│   Source    │ →   │   Chunker    │ →   │   Embedder    │ →   │ Vector Store │
│  (Files)    │     │ (Semantic)   │     │  (API/Local)  │     │   (JSON)     │
└─────────────┘     └──────────────┘     └───────────────┘     └──────────────┘
                                                                        ↓
┌─────────────┐     ┌──────────────┐     ┌───────────────┐           ↓
│   Answer    │ ←   │     LLM      │ ←   │    Search     │ ←─────────┘
│             │     │  (Synthesis) │     │  (Similarity) │
└─────────────┘     └──────────────┘     └───────────────┘
```

### components

- **loader** (`loader.go`): reads source files from disk, filters by extension
- **chunker** (`chunker.go`): intelligently splits code by functions/methods,
  markdown by headers
- **embedder** (`llm.go`, `openai.go`, `anthropic.go`, `voyage.go`): generates
  vector embeddings
- **index store** (`vectorstore.go`): compressed storage (.lrindex) with cosine
  similarity search
- **multi-source** (`multisource.go`): queries across multiple indexed
  repositories
- **rag** (`rag.go`): retrieval-augmented generation pipeline
- **mcp server** (`mcp.go`): model context protocol integration for claude code

### data flow

1. **indexing**: files → chunks → embeddings → compressed storage (.lrindex)
2. **querying**: question → embedding → similarity search → context retrieval →
   llm synthesis

## data storage

localrag follows the xdg base directory specification:

- **indexes**: `~/.local/share/lr/indexes/` (or `$XDG_DATA_HOME/lr/indexes`)
- **config**: `~/.config/lr/` (or `$XDG_CONFIG_HOME/lr`)
- **env file**: checks current directory first, then `~/.config/lr/env`

indexes are stored in compressed `.lrindex` format (gzip), providing ~50-65%
space savings over plain json.

use `lr paths` to see where your data is stored.

## setup

1. **build the binary:**

```bash
go build
```

this creates the `lr` binary.

2. **configure api keys:**

create a `.env` file:

```bash
# option a: openai only
OPENAI_API_KEY=your-key

# option b: openai + claude (recommended)
OPENAI_API_KEY=your-key
ANTHROPIC_API_KEY=your-key

# option c: voyage + claude (best for code)
VOYAGE_API_KEY=your-key
ANTHROPIC_API_KEY=your-key
```

## commands

localrag provides several commands for different workflows:

### `lr index` - index repositories

create searchable vector stores from code and documentation.

**basic usage:**

```bash
lr index --src /path/to/repo --code --out-name myproject
```

**flags:**

- `--src` (required): source directory to index
- `--code`: index code files (.go, .js, .ts, .jsx, .tsx, .templ)
- `--docs`: index markdown documentation (.md)
- `--out`: exact output path (e.g., `vectorstore/custom.json`)
- `--out-name`: output name with auto-timestamp (e.g., `myproject` →
  `vectorstore/myproject_20250109.json`)
- `--dry-run`: preview what would be indexed without actually indexing
- `--max-file-size`: maximum file size in bytes (default: 100KB)
- `--split-large`: split large files into sections instead of skipping
- `--include-tests`: include test files (useful for usage examples)

**examples:**

```bash
# index code only
lr index --src ./myproject --code --out-name myproject

# index both code and docs
lr index --src ./myproject --code --docs --out-name myproject

# index with custom file size limit
lr index --src ./large-repo --code --max-file-size 200000 --split-large --out-name largerepo

# dry run to see what would be indexed
lr index --src ./myproject --code --docs --dry-run
```

**output:** creates compressed index at
`~/.local/share/lr/indexes/{name}_{timestamp}.lrindex`

### `lr query` - query indexed repositories

ask questions about your indexed code and documentation.

**basic usage:**

```bash
lr query "how does authentication work?"
```

**flags:**

- `--top-k`: number of relevant chunks to retrieve (default: 3)
- `--sources`: filter by specific source names (comma-separated)
- `--use-mcp`: use running mcp server instead of loading indexes directly
- `--no-synthesize`: return raw chunks without llm synthesis (only with
  `--use-mcp`)

**standard mode (default):**

```bash
# loads indexes from disk, synthesizes answer with llm
lr query "how to create a consumer?"

# retrieve more context
lr query "explain the stream configuration" --top-k 5

# query specific sources only
lr query "jetstream examples" --sources nats-go,docs
```

**mcp mode (faster for repeated queries):**

```bash
# uses running mcp server (no reload overhead)
lr query "how to create a consumer?" --use-mcp

# get raw chunks without llm synthesis (cheaper, faster)
lr query "what is a stream?" --use-mcp --no-synthesize

# more chunks for better context
lr query "jetstream configuration" --use-mcp --top-k 5
```

**why use `--use-mcp`?**

- **faster**: mcp server preloads indexes once, subsequent queries are 10-100x
  faster
- **cheaper**: with `--no-synthesize`, only pays for embeddings (not chat
  completion)
- **flexible**: can run multiple queries without reloading indexes each time
- **shareable**: multiple scripts/tools can query the same running mcp server

**typical workflow:**

```bash
# terminal 1: start mcp server (preloads indexes)
lr mcp

# terminal 2: run fast queries
lr query "what is jetstream?" --use-mcp
lr query "how do consumers work?" --use-mcp
lr query "stream configuration options?" --use-mcp --top-k 5

# get raw chunks for your own processing
lr query "consumer examples" --use-mcp --no-synthesize | grep -A 10 "pull consumer"
```

### `lr interactive` - interactive query mode

start an interactive session for asking multiple questions.

**usage:**

```bash
lr interactive
```

type `exit` or `quit` to end the session.

### `lr list` - list indexed repositories

display all available vector store indexes with metadata.

**usage:**

```bash
lr list
```

**output:**

```
found 5 vector store(s):

  • docs
    file: nats_docs_20250109.json
    chunks: 1234
    files indexed: 156
    source: /path/to/nats.docs
    indexed: 2025-01-09T10:30:00Z

  • nats-go
    file: nats_nats-go_20250109.json
    chunks: 2456
    ...
```

### `lr mcp` - mcp server for ai agents

start a model context protocol server for integration with ai agents (claude
code, chatgpt, etc).

**usage:**

```bash
lr mcp
```

**flags:**

- `--no-preload`: disable vector store preloading (allows on-the-fly updates)
- `--reload <pid>`: send reload signal to mcp server with given pid
- `--reload-all`: send reload signal to all running lr mcp processes

**default behavior (preloading enabled):**

- loads all vector stores into memory at startup
- provides 10-100x faster query responses
- use `--reload-all` or `--reload <pid>` to pick up newly indexed repositories

**reloading indexes:**

when you add new indexes, you can reload running mcp servers without restarting:

```bash
# reload all running lr mcp processes
lr mcp --reload-all

# reload a specific mcp server by pid
lr mcp --reload 12345
```

the mcp server prints its pid at startup for easy reference.

**with `--no-preload`:**

- loads vector stores on each query
- allows updating indexes without restarting the server
- useful during active development

**mcp tool parameters:**

when calling the `query_repositories` tool, ai agents can pass:

- `query` (required): the question to ask
- `top_k` (optional): number of chunks to retrieve (default: 3)
- `synthesize` (optional): whether to synthesize an answer using llm (default:
  true)
  - `true`: uses llm to generate a cohesive answer from chunks (costs more,
    better answers)
  - `false`: returns raw chunks only (faster, cheaper, lets the calling agent
    synthesize)

**ai agent integration:**

<details>
<summary>claude code setup</summary>

use the claude code cli to register the mcp server:

```bash
claude mcp add --transport stdio lr -- /path/to/lr mcp
```

replace `/path/to/lr` with the actual path to your lr binary (e.g.,
`~/go/bin/lr` or the output of `which lr`).

alternatively, run `lr setup` to see the exact command with your binary path:

```bash
lr setup
```

after running the command, restart claude code to activate the integration.

</details>

<details>
<summary>chatgpt setup (plus/pro/enterprise)</summary>

chatgpt added mcp support in september 2025:

1. enable developer mode: settings → connectors → advanced → developer mode
2. add your mcp server configuration through the chatgpt web interface
3. chatgpt can now query your indexed repositories

**recommended**: set `synthesize: false` when calling from chatgpt to avoid
double llm costs (chatgpt will synthesize the answer itself from the raw
chunks).

</details>

### `lr setup` - print mcp configuration

print the mcp server configuration for easy setup with ai agents.

**usage:**

```bash
lr setup
```

**output:**

- shows your lr binary path
- prints ready-to-use json configuration
- displays setup instructions

**example:**

```bash
$ lr setup
=== claude code mcp setup ===

config file: /Users/you/.config/claude/claude_desktop_config.json

configuration:
{
  "mcpServers": {
    "nats-rag": {
      "command": "/path/to/lr",
      "args": ["mcp"]
    }
  }
}
```

### `lr paths` - show data directories

display where lr stores indexes and configuration.

**usage:**

```bash
lr paths
```

**output:**

```bash
=== lr data directories ===

indexes:  /Users/you/.local/share/lr/indexes
config:   /Users/you/.config/lr
env file: .env

these directories follow the XDG base directory specification
you can override them with environment variables:
  XDG_DATA_HOME   - base directory for data files
  XDG_CONFIG_HOME - base directory for config files
```

## query modes comparison

| mode                | command                                  | speed                | cost                         | when to use                   |
| ------------------- | ---------------------------------------- | -------------------- | ---------------------------- | ----------------------------- |
| **standard**        | `lr query "question"`                    | slow (loads indexes) | moderate (embeddings + chat) | one-off queries, testing      |
| **interactive**     | `lr interactive`                         | medium (loads once)  | moderate (embeddings + chat) | multiple questions in session |
| **mcp + synthesis** | `lr query "q" --use-mcp`                 | fast (preloaded)     | moderate (embeddings + chat) | repeated queries, automation  |
| **mcp raw**         | `lr query "q" --use-mcp --no-synthesize` | fastest              | cheapest (embeddings only)   | scripts, ai agent integration |

**recommendation:**

- **for ai agents** (claude code, chatgpt): use mcp with `synthesize=false` -
  let the agent synthesize
- **for cli automation**: use `--use-mcp --no-synthesize` - fast and cheap
- **for exploration**: use standard or interactive mode - easy and
  straightforward
- **for production tools**: use mcp server with preloading - maximum performance

## use cases

### 1. ai agent integration (recommended)

integrate your indexed repositories with ai agents like claude code or chatgpt.

**benefits:**

- ai agents can query your private codebases during conversations
- no need to copy/paste code into chat windows
- keeps proprietary code local and secure
- agents get accurate, up-to-date information from your actual codebase

**example conversation with claude code:**

```
you: "show me how to create a pull consumer in go"
claude: [calls query_repositories tool with synthesize=false]
        [receives raw code chunks from nats-go repository]
        [synthesizes explanation using the actual code]
        "here's how to create a pull consumer in go..."
```

### 2. fast cli queries with mcp mode

use `--use-mcp` to query a long-running mcp server for instant results.

**benefits:**

- **10-100x faster** than reloading indexes each time
- perfect for scripts and automation
- share one mcp server across multiple tools/terminals
- reduce api costs with `--no-synthesize`

**example script:**

```bash
#!/bin/bash
# fast batch queries using shared mcp server

# ensure mcp server is running (start if not)
lr mcp &
MCP_PID=$!

# run multiple fast queries
lr query "consumer configuration" --use-mcp --no-synthesize > consumers.txt
lr query "stream limits" --use-mcp --no-synthesize > streams.txt
lr query "jetstream examples" --use-mcp --no-synthesize > examples.txt

# process results
grep -i "max" *.txt

kill $MCP_PID
```

### 3. private rag for documentation

index your company's internal documentation and code for team-wide search.

**benefits:**

- keep sensitive docs and code on-premises
- no data sent to third parties (except embeddings api)
- fast semantic search across large codebases
- consistent answers based on actual source code

**example workflow:**

```bash
# index your internal repos
lr index --src ~/company/backend --code --out-name backend
lr index --src ~/company/docs --docs --out-name internal-docs
lr index --src ~/company/frontend --code --out-name frontend

# query across all repos
lr query "authentication flow" --top-k 5

# or use with your team's ai tools via mcp
```

### 4. learning and exploration

explore unfamiliar codebases quickly with natural language queries.

**benefits:**

- ask questions in plain english instead of grepping
- get context-aware answers with source citations
- discover related code patterns
- understand architecture without reading everything

**example:**

```bash
# exploring nats codebase
lr query "how does message routing work?"
lr query "difference between core nats and jetstream"
lr query "examples of error handling patterns"
```

## how it works

### indexing pipeline

1. **loading**: reads source files matching specified extensions
2. **filtering**: skips files based on size, test patterns, or paths
3. **chunking**: intelligently splits content:
   - **code**: by function/method boundaries with context
   - **markdown**: by headers while preserving structure
4. **embedding**: generates vector embeddings via api
5. **storage**: saves chunks with embeddings to json
6. **checkpointing**: periodically saves progress (resume on failure)

### query pipeline

1. **embedding**: converts question to vector embedding
2. **search**: finds top-k most similar chunks via cosine similarity
3. **ranking**: scores chunks across all loaded vector stores
4. **context building**: assembles relevant chunks with metadata
5. **synthesis**: llm generates answer with source citations
6. **formatting**: returns answer with similarity scores

## project structure

```
.
├── main.go              # cli commands and flags
├── mcp.go               # mcp server implementation
├── mcpclient.go         # mcp client for --use-mcp queries
├── paths.go             # xdg directory paths
├── loader.go            # file loading with filtering
├── chunker.go           # semantic chunking (code/markdown)
├── vectorstore.go       # compressed index storage (.lrindex)
├── multisource.go       # multi-repository querying
├── rag.go               # retrieval-augmented generation
├── llm.go               # llm client interface
├── openai.go            # openai embeddings + chat
├── anthropic.go         # claude chat client
├── voyage.go            # voyage ai embeddings
└── env.go               # .env file loader
```

**data stored in xdg directories:**

- indexes: `~/.local/share/lr/indexes/` (compressed .lrindex files)
- config: `~/.config/lr/`

### key files explained

- **main.go**: cobra cli setup, command routing, flag definitions
- **mcp.go**: mcp protocol server with preloading support for ai agents
- **mcpclient.go**: mcp client implementation for --use-mcp queries
- **paths.go**: xdg directory path handling
- **loader.go**: recursive file discovery, extension filtering, size limits
- **chunker.go**: splits code by functions/classes, markdown by headers
- **vectorstore.go**: compressed index storage (.lrindex), cosine similarity
  search
- **multisource.go**: aggregates searches across multiple indexes
- **rag.go**: combines retrieval + llm synthesis with context building
- **llm.go**: interface for embeddings and chat (provider-agnostic)
- **{openai,anthropic,voyage}.go**: provider-specific api implementations

## supported file types

- **code**: `.go`, `.js`, `.ts`, `.jsx`, `.tsx`, `.templ`, `.py`, `.java`, `.c`, `.h`
- **documentation**: `.md`

## example workflow

### indexing nats documentation

```bash
# index nats go client
lr index --src ~/code/nats.go --code --out-name nats-go

# index nats.js client
lr index --src ~/code/nats.js --code --out-name nats-js

# index nats documentation
lr index --src ~/code/nats.docs --docs --out-name docs

# list what's indexed
lr list
```

### querying from cli

```bash
# ask a question
lr query "how do i create a jetstream consumer?"

# get more context
lr query "explain pinned consumers" --top-k 5

# query specific repos only
lr query "javascript examples" --sources nats-js,docs
```

### using with claude code

```bash
# start mcp server with preloading (fast)
lr mcp

# or without preloading (allows on-the-fly updates)
lr mcp --no-preload
```

then in claude code, just ask questions naturally:

- "how does nats authentication work?"
- "show me an example of a pinned consumer in javascript"
- "what are the jetstream configuration options?"

## performance tips

### for indexing

- use `--dry-run` first to estimate time and cost
- increase `--max-file-size` for larger codebases
- enable `--split-large` to include files that exceed size limits
- checkpoint files auto-save progress every 100 chunks

### for querying

- use **preloading** (default) for production mcp servers
- use `--no-preload` only during active development
- adjust `--top-k` based on context needs (3-5 is usually optimal)
- filter by `--sources` to reduce search space

### index optimization

- compressed .lrindex format provides ~50-65% space savings
- keep indexes under 10k chunks for fast searches
- split large repositories into logical components
- rebuild indexes periodically as code evolves
- indexes are automatically gzip compressed for faster i/o

## roadmap

- [ ] url support for `--src` (auto-clone repos)
- [ ] more languages (rust, c++, c#, ruby, php)
- [ ] ast-based parsing for better chunking
- [ ] incremental updates (only re-index changed files)
- [ ] parallel embedding generation
- [ ] semantic caching for common queries
- [ ] faiss or similar for approximate nearest neighbor search

## license

tbd
