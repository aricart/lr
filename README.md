# LocalRag (lr)

A local-first retrieval-augmented generation (rag) system for indexing and
querying code repositories and documentation.

All the code in this project is AI generated by
[claude code](https://claude.ai). Of course all the ideas on how to make it work
are mine. The idea is that this can enhance development of code written by
humans or tools with factual and almost hallucination-free constructs.

## why localrag?

- **privacy-first**: all data stays on your machine, no code sent to third
  parties
- **flexible**: index any repository - your code, open source projects,
  documentation
- **simple**: single binary, no databases or complex dependencies
- **code-aware**: intelligent chunking that understands functions and methods
- **multi-source**: query across multiple indexed repositories at once
- **customizable**: supports multiple embedding and llm providers

## features

- index local directories or git repositories
- intelligent code-aware chunking for go, javascript, typescript, templ, python,
  java, and c
- markdown documentation support
- multiple embedding providers:
  - openai (text-embedding-3-small)
  - voyage ai (code-optimized embeddings)
  - ollama (local embeddings, no api key needed)
- multiple llm providers:
  - openai (gpt-4o-mini)
  - anthropic claude (sonnet 4)
- compressed index storage (.lrindex format with gzip)
- xdg base directory compliance for data and config
- multi-source search (query across all indexed repos)
- interactive cli mode
- model context protocol (mcp) server for ai agent integration
- checkpoint/resume support for long indexing jobs
- incremental updates (only re-index changed files via git or mtime detection)
- bulk update all indexes with automatic backup
- code review mode with local ollama embeddings and file watching

## architecture

localrag is built around a simple but powerful pipeline:

```
┌─────────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐
│   Source    │ →   │   Chunker    │ →   │   Embedder    │ →   │ Vector Store │
│  (Files)    │     │ (Semantic)   │     │  (API/Local)  │     │   (JSON)     │
└─────────────┘     └──────────────┘     └───────────────┘     └──────────────┘
                                                                        ↓
┌─────────────┐     ┌──────────────┐     ┌───────────────┐           ↓
│   Answer    │ ←   │     LLM      │ ←   │    Search     │ ←─────────┘
│             │     │  (Synthesis) │     │  (Similarity) │
└─────────────┘     └──────────────┘     └───────────────┘
```

### components

- **loader** (`loader.go`): reads source files from disk, filters by extension
- **chunker** (`chunker.go`): intelligently splits code by functions/methods,
  markdown by headers
- **embedder** (`llm.go`, `openai.go`, `anthropic.go`, `voyage.go`): generates
  vector embeddings
- **index store** (`vectorstore.go`): compressed storage (.lrindex) with cosine
  similarity search
- **multi-source** (`multisource.go`): queries across multiple indexed
  repositories
- **rag** (`rag.go`): retrieval-augmented generation pipeline
- **mcp server** (`mcp.go`): model context protocol integration for claude code

### data flow

1. **indexing**: files → chunks → embeddings → compressed storage (.lrindex)
2. **querying**: question → embedding → similarity search → context retrieval →
   llm synthesis

## data storage

localrag follows the xdg base directory specification:

- **indexes**: `~/.local/share/lr/indexes/` (or `$XDG_DATA_HOME/lr/indexes`)
- **config**: `~/.config/lr/` (or `$XDG_CONFIG_HOME/lr`)
- **env file**: checks current directory first, then `~/.config/lr/env`

indexes are stored in compressed `.lrindex` format (gzip), providing ~50-65%
space savings over plain json.

use `lr paths` to see where your data is stored.

## setup

1. **build the binary:**

```bash
go build
```

this creates the `lr` binary.

2. **configure api keys:**

create a `.env` file:

```bash
# option a: openai only
OPENAI_API_KEY=your-key

# option b: openai + claude (recommended)
OPENAI_API_KEY=your-key
ANTHROPIC_API_KEY=your-key

# option c: voyage + claude (best for code)
VOYAGE_API_KEY=your-key
ANTHROPIC_API_KEY=your-key

# option d: ollama (local embeddings, no api key for embeddings)
# just need ANTHROPIC_API_KEY for chat synthesis
ANTHROPIC_API_KEY=your-key
```

## global flags

these flags work with any command:

- `--embedding-model`: embedding provider (aliases: `openai`, `voyage`,
  `voyage3`, `ollama`)
- `--model`: chat model (aliases: `sonnet`, `haiku`, `opus`, `gpt-4o`,
  `gpt-4o-mini`)

**examples:**

```bash
# use voyage embeddings with claude opus
lr query "how does auth work?" --embedding-model voyage --model opus

# use openai for everything
lr index --src ./repo --out-name repo --embedding-model openai --model gpt-4o
```

## private/sensitive data

for sensitive documents that should never leave your machine, use ollama for
fully local embeddings:

```bash
# index sensitive code with local embeddings
lr index --src ./private-repo --out-name private --embedding-model=ollama

# query locally - embeddings never leave your machine
lr query --sources=private --embedding-model=ollama "find auth vulnerabilities"
```

**important**: indexes are model-specific by design. an index created with
ollama can only be queried with ollama. this prevents accidentally exposing
sensitive data to external apis - queries with mismatched models return no
results.

use `lr list` to see which embedding model each index uses:

```
lr list
  • private_20251215.lrindex
    embedding: nomic-embed-text ✓    # compatible with current --embedding-model

  • public-repo_20251215.lrindex
    embedding: text-embedding-3-small ✗  # incompatible (would need openai)
```

## commands

localrag provides several commands for different workflows:

### `lr index` - index repositories

create searchable vector stores from code and documentation.

**basic usage:**

```bash
lr index --src /path/to/repo --out-name myproject
```

**flags:**

- `--src` (required): source directory to index
- `--code`: index code files (.go, .js, .ts, .jsx, .tsx, .templ) [default: true]
- `--docs`: index markdown documentation (.md) [default: true]
- `--include-tests`: include test files (useful for usage examples) [default: true]
- `--out`: exact output path (e.g., `vectorstore/custom.json`)
- `--out-name`: output name with auto-timestamp (e.g., `myproject` →
  `vectorstore/myproject_20250109.json`)
- `--dry-run`: preview what would be indexed without actually indexing
- `--max-file-size`: maximum file size in bytes (default: 100KB)
- `--split-large`: split large files into sections instead of skipping
- `--update`: incrementally update existing index (only re-index changed files)
- `--git`: use git to detect changes (default: file mtime)

**examples:**

```bash
# index code, docs, and tests (default)
lr index --src ./myproject --out-name myproject

# index code only (no docs)
lr index --src ./myproject --docs=false --out-name myproject

# index docs only (no code)
lr index --src ./myproject --code=false --out-name myproject

# exclude test files
lr index --src ./myproject --include-tests=false --out-name myproject

# index with custom file size limit
lr index --src ./large-repo --max-file-size 200000 --split-large --out-name largerepo

# dry run to see what would be indexed
lr index --src ./myproject --dry-run

# incrementally update an existing index (only changed files)
lr index --src ./myproject --out-name myproject --update
```

**output:** creates compressed index at
`~/.local/share/lr/indexes/{name}_{timestamp}.lrindex`

### `lr query` - query indexed repositories

ask questions about your indexed code and documentation.

**basic usage:**

```bash
lr query "how does authentication work?"
```

**flags:**

- `--top-k`: number of relevant chunks to retrieve (default: 3)
- `--sources`: filter by specific source names (comma-separated)
- `--use-mcp`: use running mcp server instead of loading indexes directly
- `--no-synthesize`: return raw chunks without llm synthesis (only with
  `--use-mcp`)

**standard mode (default):**

```bash
# loads indexes from disk, synthesizes answer with llm
lr query "how to create a consumer?"

# retrieve more context
lr query "explain the stream configuration" --top-k 5

# query specific sources only
lr query "jetstream examples" --sources nats-go,docs
```

**mcp mode (faster for repeated queries):**

```bash
# uses running mcp server (no reload overhead)
lr query "how to create a consumer?" --use-mcp

# get raw chunks without llm synthesis (cheaper, faster)
lr query "what is a stream?" --use-mcp --no-synthesize

# more chunks for better context
lr query "jetstream configuration" --use-mcp --top-k 5
```

**why use `--use-mcp`?**

- **faster**: mcp server preloads indexes once, subsequent queries are 10-100x
  faster
- **cheaper**: with `--no-synthesize`, only pays for embeddings (not chat
  completion)
- **flexible**: can run multiple queries without reloading indexes each time
- **shareable**: multiple scripts/tools can query the same running mcp server

**typical workflow:**

```bash
# terminal 1: start mcp server (preloads indexes)
lr mcp

# terminal 2: run fast queries
lr query "what is jetstream?" --use-mcp
lr query "how do consumers work?" --use-mcp
lr query "stream configuration options?" --use-mcp --top-k 5

# get raw chunks for your own processing
lr query "consumer examples" --use-mcp --no-synthesize | grep -A 10 "pull consumer"
```

### `lr interactive` - interactive query mode

start an interactive session for asking multiple questions.

**usage:**

```bash
lr interactive
```

type `exit` or `quit` to end the session.

### `lr list` - list indexed repositories

display all available vector store indexes with metadata.

**usage:**

```bash
lr list
```

**output:**

```
found 5 vector store(s):

  • docs
    file: nats_docs_20250109.json
    chunks: 1234
    files indexed: 156
    source: /path/to/nats.docs
    indexed: 2025-01-09T10:30:00Z

  • nats-go
    file: nats_nats-go_20250109.json
    chunks: 2456
    ...
```

### `lr mcp` - mcp server for ai agents

start a model context protocol server for integration with ai agents (claude
code, chatgpt, etc).

**usage:**

```bash
lr mcp
```

**flags:**

- `--no-preload`: disable vector store preloading (allows on-the-fly updates)
- `--reload <pid>`: send reload signal to mcp server with given pid
- `--reload-all`: send reload signal to all running lr mcp processes

**default behavior (preloading enabled):**

- loads all vector stores into memory at startup
- provides 10-100x faster query responses
- use `--reload-all` or `--reload <pid>` to pick up newly indexed repositories

**reloading indexes:**

when you add new indexes, you can reload running mcp servers without restarting:

```bash
# reload all running lr mcp processes
lr mcp --reload-all

# reload a specific mcp server by pid
lr mcp --reload 12345
```

the mcp server prints its pid at startup for easy reference.

**with `--no-preload`:**

- loads vector stores on each query
- allows updating indexes without restarting the server
- useful during active development

**mcp tools:**

the mcp server exposes five tools for ai agents:

| tool                 | description                                          |
| -------------------- | ---------------------------------------------------- |
| `query_repositories` | semantic search across all indexed repos             |
| `list_indexes`       | list all available indexes with metadata             |
| `get_index_stats`    | detailed statistics for a specific index             |
| `search_by_file`     | get all chunks from a specific file path             |
| `get_diff_context`   | git diff with indexed context for code review        |

**query_repositories parameters:**

- `query` (required): the question to ask
- `top_k` (optional): number of chunks to retrieve (default: 3)
- `synthesize` (optional): whether to synthesize an answer using llm (default:
  true)
  - `true`: uses llm to generate a cohesive answer from chunks (costs more,
    better answers)
  - `false`: returns raw chunks only (faster, cheaper, lets the calling agent
    synthesize)
- `sources` (optional): comma-separated list of source names to search (e.g.,
  'jwt,nats-server'). if not specified, searches all sources

**get_index_stats parameters:**

- `name` (required): the index name (e.g., 'nats-server', 'docs')

**search_by_file parameters:**

- `path` (required): file path to search for (can be partial, e.g., 'server.go')

**get_diff_context parameters:**

- `top_k` (optional): number of context chunks per changed file (default: 3)
- `uncommitted_only` (optional): only show uncommitted/staged changes instead of
  full branch diff (default: false)

by default, shows all changes on current branch vs main/master. requires an
active review session started with `lr review start`.

**ai agent integration:**

<details>
<summary>claude code setup</summary>

use the claude code cli to register the mcp server:

```bash
claude mcp add --transport stdio lr -- /path/to/lr mcp
```

replace `/path/to/lr` with the actual path to your lr binary (e.g.,
`~/go/bin/lr` or the output of `which lr`).

alternatively, run `lr setup` to see the exact command with your binary path:

```bash
lr setup
```

after running the command, restart claude code to activate the integration.

</details>

<details>
<summary>chatgpt setup (plus/pro/enterprise)</summary>

chatgpt added mcp support in september 2025:

1. enable developer mode: settings → connectors → advanced → developer mode
2. add your mcp server configuration through the chatgpt web interface
3. chatgpt can now query your indexed repositories

**recommended**: set `synthesize: false` when calling from chatgpt to avoid
double llm costs (chatgpt will synthesize the answer itself from the raw
chunks).

</details>

### `lr setup` - print mcp configuration

print the mcp server configuration for easy setup with ai agents.

**usage:**

```bash
lr setup
```

**output:**

- shows your lr binary path
- prints ready-to-use json configuration
- displays setup instructions

**example:**

```bash
$ lr setup
=== claude code mcp setup ===

config file: /Users/you/.config/claude/claude_desktop_config.json

configuration:
{
  "mcpServers": {
    "nats-rag": {
      "command": "/path/to/lr",
      "args": ["mcp"]
    }
  }
}
```

### `lr paths` - show data directories

display where lr stores indexes and configuration.

**usage:**

```bash
lr paths
```

**output:**

```bash
=== lr data directories ===

indexes:  /Users/you/.local/share/lr/indexes
config:   /Users/you/.config/lr
env file: .env

these directories follow the XDG base directory specification
you can override them with environment variables:
  XDG_DATA_HOME   - base directory for data files
  XDG_CONFIG_HOME - base directory for config files
```

### `lr review` - code review with local embeddings

start a review session that indexes your project locally using ollama for
embeddings. provides context-aware code review through the mcp `get_diff_context`
tool.

**subcommands:**

- `lr review start`: start a review session (indexes current directory, starts
  file watching)
- `lr review stop`: stop the session and delete the temporary index
- `lr review status`: show current session status
- `lr review watch`: restart file watching for an existing session

**usage:**

```bash
# start a review session in your project directory
cd /path/to/your/project
lr review start
```

**what it does:**

1. starts ollama if not running
2. pulls the embedding model (nomic-embed-text) if needed
3. indexes all code and docs in the current directory
4. watches for file changes and updates the index in real-time
5. enables the `get_diff_context` mcp tool for ai agents

**requirements:**

- [ollama](https://ollama.ai) installed locally
- no api keys needed (uses local embeddings only)

**example workflow with claude code:**

```bash
# terminal 1: start review session
cd ~/myproject
lr review start

# in claude code, ask for a code review
# claude will use get_diff_context to see your changes with relevant context
```

**stopping the session:**

```bash
# stop and clean up (also triggered by Ctrl+C in the terminal running start)
lr review stop
```

**notes:**

- the review index is temporary and stored separately from regular indexes
- file watching automatically re-indexes changed files within 500ms
- stale sessions (from crashes) are automatically cleaned up on next start

### `lr update-all` - bulk update all indexes

incrementally update all indexes that have recorded source paths. creates a
backup before making any changes.

**usage:**

```bash
lr update-all
```

**flags:**

- `--git`: force git-based change detection (default: auto-detect)

**what it does:**

1. **scans** all indexes and detects changes (without modifying anything)
2. **shows summary** of what needs updating and git warnings
3. **creates backup** of all indexes before any updates
4. **updates** only indexes that have changes

**example output:**

```
scanning indexes for changes...
  - old-index.lrindex: no source path
  ✓ nats-server_20251109.lrindex: /path/to/nats-server

=== SCAN RESULTS ===
  ✓ nats-server: 2 added, 48 modified, 0 deleted
  - nats-go: up to date

=== GIT WARNINGS ===
  ⚠ nats-go: 4 commits behind remote (consider: cd /path/to/nats.go && git pull)

1 index(es) need updating with 50 total file changes

creating backup in backup_20251215_201550...
backed up 10 index files

updating indexes...
```

**notes:**

- indexes without source paths are skipped (re-index from scratch to add path)
- auto-uses git-based detection if index has `LastCommit` metadata
- backup directory is kept after completion for safety
- if no changes detected, exits early without creating backup

## query modes comparison

| mode                | command                                  | speed                | cost                         | when to use                   |
| ------------------- | ---------------------------------------- | -------------------- | ---------------------------- | ----------------------------- |
| **standard**        | `lr query "question"`                    | slow (loads indexes) | moderate (embeddings + chat) | one-off queries, testing      |
| **interactive**     | `lr interactive`                         | medium (loads once)  | moderate (embeddings + chat) | multiple questions in session |
| **mcp + synthesis** | `lr query "q" --use-mcp`                 | fast (preloaded)     | moderate (embeddings + chat) | repeated queries, automation  |
| **mcp raw**         | `lr query "q" --use-mcp --no-synthesize` | fastest              | cheapest (embeddings only)   | scripts, ai agent integration |

**recommendation:**

- **for ai agents** (claude code, chatgpt): use mcp with `synthesize=false` -
  let the agent synthesize
- **for cli automation**: use `--use-mcp --no-synthesize` - fast and cheap
- **for exploration**: use standard or interactive mode - easy and
  straightforward
- **for production tools**: use mcp server with preloading - maximum performance

## use cases

### 1. ai agent integration (recommended)

integrate your indexed repositories with ai agents like claude code or chatgpt.

**benefits:**

- ai agents can query your private codebases during conversations
- no need to copy/paste code into chat windows
- keeps proprietary code local and secure
- agents get accurate, up-to-date information from your actual codebase

**example conversation with claude code:**

```
you: "show me how to create a pull consumer in go"
claude: [calls query_repositories tool with synthesize=false]
        [receives raw code chunks from nats-go repository]
        [synthesizes explanation using the actual code]
        "here's how to create a pull consumer in go..."
```

### 2. fast cli queries with mcp mode

use `--use-mcp` to query a long-running mcp server for instant results.

**benefits:**

- **10-100x faster** than reloading indexes each time
- perfect for scripts and automation
- share one mcp server across multiple tools/terminals
- reduce api costs with `--no-synthesize`

**example script:**

```bash
#!/bin/bash
# fast batch queries using shared mcp server

# ensure mcp server is running (start if not)
lr mcp &
MCP_PID=$!

# run multiple fast queries
lr query "consumer configuration" --use-mcp --no-synthesize > consumers.txt
lr query "stream limits" --use-mcp --no-synthesize > streams.txt
lr query "jetstream examples" --use-mcp --no-synthesize > examples.txt

# process results
grep -i "max" *.txt

kill $MCP_PID
```

### 3. private rag for documentation

index your company's internal documentation and code for team-wide search.

**benefits:**

- keep sensitive docs and code on-premises
- no data sent to third parties (except embeddings api)
- fast semantic search across large codebases
- consistent answers based on actual source code

**example workflow:**

```bash
# index your internal repos (code, docs, tests included by default)
lr index --src ~/company/backend --out-name backend
lr index --src ~/company/docs --out-name internal-docs
lr index --src ~/company/frontend --out-name frontend

# query across all repos
lr query "authentication flow" --top-k 5

# or use with your team's ai tools via mcp
```

### 4. learning and exploration

explore unfamiliar codebases quickly with natural language queries.

**benefits:**

- ask questions in plain english instead of grepping
- get context-aware answers with source citations
- discover related code patterns
- understand architecture without reading everything

**example:**

```bash
# exploring nats codebase
lr query "how does message routing work?"
lr query "difference between core nats and jetstream"
lr query "examples of error handling patterns"
```

## how it works

### indexing pipeline

1. **loading**: reads source files matching specified extensions
2. **filtering**: skips files based on size, test patterns, or paths
3. **chunking**: intelligently splits content:
   - **code**: by function/method boundaries with context
   - **markdown**: by headers while preserving structure
4. **embedding**: generates vector embeddings via api
5. **storage**: saves chunks with embeddings to json
6. **checkpointing**: periodically saves progress (resume on failure)

### query pipeline

1. **embedding**: converts question to vector embedding
2. **search**: finds top-k most similar chunks via cosine similarity
3. **ranking**: scores chunks across all loaded vector stores
4. **context building**: assembles relevant chunks with metadata
5. **synthesis**: llm generates answer with source citations
6. **formatting**: returns answer with similarity scores

## project structure

```
.
├── main.go              # cli commands and flags
├── mcp.go               # mcp server implementation
├── mcpclient.go         # mcp client for --use-mcp queries
├── paths.go             # xdg directory paths
├── loader.go            # file loading with filtering
├── chunker.go           # semantic chunking (code/markdown)
├── incremental.go       # incremental update detection (git/mtime)
├── vectorstore.go       # compressed index storage (.lrindex)
├── multisource.go       # multi-repository querying
├── rag.go               # retrieval-augmented generation
├── llm.go               # llm client interface
├── openai.go            # openai embeddings + chat
├── anthropic.go         # claude chat client
├── voyage.go            # voyage ai embeddings
├── ollama.go            # ollama local embeddings
├── review.go            # code review session management
└── env.go               # .env file loader
```

**data stored in xdg directories:**

- indexes: `~/.local/share/lr/indexes/` (compressed .lrindex files)
- config: `~/.config/lr/`

### key files explained

- **main.go**: cobra cli setup, command routing, flag definitions
- **mcp.go**: mcp protocol server with preloading support for ai agents
- **mcpclient.go**: mcp client implementation for --use-mcp queries
- **paths.go**: xdg directory path handling
- **loader.go**: recursive file discovery, extension filtering, size limits
- **chunker.go**: splits code by functions/classes, markdown by headers
- **incremental.go**: change detection via git diff or file mtime, atomic saves
- **vectorstore.go**: compressed index storage (.lrindex), cosine similarity
  search
- **multisource.go**: aggregates searches across multiple indexes
- **rag.go**: combines retrieval + llm synthesis with context building
- **llm.go**: interface for embeddings and chat (provider-agnostic)
- **{openai,anthropic,voyage,ollama}.go**: provider-specific api implementations
- **review.go**: code review session with ollama embeddings and file watching

## supported file types

- **code**: `.go`, `.js`, `.ts`, `.jsx`, `.tsx`, `.templ`, `.py`, `.java`, `.c`,
  `.h`
- **documentation**: `.md`

## example workflow

### indexing nats documentation

```bash
# index nats go client
lr index --src ~/code/nats.go --out-name nats-go

# index nats.js client
lr index --src ~/code/nats.js --out-name nats-js

# index nats documentation
lr index --src ~/code/nats.docs --out-name docs

# list what's indexed
lr list
```

### querying from cli

```bash
# ask a question
lr query "how do i create a jetstream consumer?"

# get more context
lr query "explain pinned consumers" --top-k 5

# query specific repos only
lr query "javascript examples" --sources nats-js,docs
```

### using with claude code

```bash
# start mcp server with preloading (fast)
lr mcp

# or without preloading (allows on-the-fly updates)
lr mcp --no-preload
```

then in claude code, just ask questions naturally:

- "how does nats authentication work?"
- "show me an example of a pinned consumer in javascript"
- "what are the jetstream configuration options?"

## performance tips

### for indexing

- use `--dry-run` first to estimate time and cost
- increase `--max-file-size` for larger codebases
- enable `--split-large` to include files that exceed size limits
- checkpoint files auto-save progress every 100 chunks

### for querying

- use **preloading** (default) for production mcp servers
- use `--no-preload` only during active development
- adjust `--top-k` based on context needs (3-5 is usually optimal)
- filter by `--sources` to reduce search space

### index optimization

- compressed .lrindex format provides ~50-65% space savings
- keep indexes under 10k chunks for fast searches
- split large repositories into logical components
- rebuild indexes periodically as code evolves
- indexes are automatically gzip compressed for faster i/o

## roadmap

- [ ] url support for `--src` (auto-clone repos)
- [ ] more languages (rust, c++, c#, ruby, php)
- [ ] ast-based parsing for better chunking
- [x] incremental updates (only re-index changed files) - `--update` flag and
      `update-all` command
- [ ] parallel embedding generation
- [ ] semantic caching for common queries
- [ ] faiss or similar for approximate nearest neighbor search

## license

tbd
